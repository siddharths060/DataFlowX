# DataFlowX: End-to-End Big Data Engineering on Azure â˜ï¸

<div align="center">

![Azure](https://img.shields.io/badge/Microsoft_Azure-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white)
![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![MongoDB](https://img.shields.io/badge/MongoDB-47A248?style=for-the-badge&logo=mongodb&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache_Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)

**A complete Azure-based data pipeline for Brazilian e-commerce analytics**

[Architecture](#architecture) â€¢ [Tech Stack](#tech-stack) â€¢ [Features](#features) â€¢ [Getting Started](#getting-started)

</div>

---

## ğŸ“‹ Overview

DataFlowX is a production-grade big data engineering project that implements a complete Azure-based data pipeline for analyzing Brazilian e-commerce data. The project follows industry-standard **Medallion Architecture** (Bronze/Silver/Gold layers) and demonstrates end-to-end data engineering workflowsâ€”from ingestion to visualization.

This project is ideal for:
- ğŸ¯ Data engineers seeking real-world Azure experience
- ğŸ“Š BI developers targeting cloud-based pipelines
- ğŸ“ Students preparing for data engineering interviews
- ğŸ’¼ Professionals building portfolio projects

### Key Highlights
- **Multi-source ingestion**: HTTP endpoints, SQL databases, and NoSQL (MongoDB)
- **Scalable architecture**: Medallion pattern with Bronze, Silver, and Gold layers
- **Real-world dataset**: Olist Brazilian E-Commerce dataset (100k+ orders)
- **Cloud-native**: Built entirely on Azure ecosystem
- **Production-ready**: Includes monitoring, error handling, and best practices

---

## ğŸ—ï¸ Architecture

### Medallion Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCES                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   GitHub    â”‚   SQL Database  â”‚         MongoDB                     â”‚
â”‚  (HTTP/CSV) â”‚   (MySQL)       â”‚   (Category Enrichment)             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚                       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Azure Data Factory â”‚
              â”‚   (Orchestration)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Azure Data Lake Gen2       â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  ğŸ¥‰ BRONZE (Raw Data)        â”‚
          â”‚     - Raw CSV files          â”‚
          â”‚     - Original schema        â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Azure Databricks   â”‚
              â”‚  (Spark Processing) â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  ğŸ¥ˆ SILVER (Cleaned Data)    â”‚
          â”‚     - Cleaned & validated    â”‚
          â”‚     - Enriched with MongoDB  â”‚
          â”‚     - Standardized formats   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Azure Databricks   â”‚
              â”‚  (Aggregations)     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  ğŸ¥‡ GOLD (Analytics Ready)   â”‚
          â”‚     - Business metrics       â”‚
          â”‚     - Aggregated tables      â”‚
          â”‚     - BI-optimized schemas   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Azure Synapse      â”‚
              â”‚  (Serving Layer)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    Power BI / Tableau       â”‚
          â”‚    (Visualization)          â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

| Technology | Purpose | Version/Details |
|------------|---------|-----------------|
| ![Azure Data Factory](https://img.shields.io/badge/-Azure_Data_Factory-0078D4?style=flat-square&logo=microsoft-azure) | **Orchestration & Ingestion** | ETL pipelines, data movement |
| ![Azure Data Lake Gen2](https://img.shields.io/badge/-Azure_Data_Lake-0078D4?style=flat-square&logo=microsoft-azure) | **Storage** | Bronze/Silver/Gold layer storage |
| ![Azure Databricks](https://img.shields.io/badge/-Databricks-FF3621?style=flat-square&logo=databricks) | **Data Processing** | Apache Spark transformations |
| ![Azure Synapse](https://img.shields.io/badge/-Synapse_Analytics-0078D4?style=flat-square&logo=microsoft-azure) | **Analytics & Serving** | External tables, analytical queries |
| ![MongoDB](https://img.shields.io/badge/-MongoDB-47A248?style=flat-square&logo=mongodb) | **Data Enrichment** | Category mapping (NoSQL) |
| ![MySQL](https://img.shields.io/badge/-MySQL-4479A1?style=flat-square&logo=mysql&logoColor=white) | **Relational Source** | Order payments data |
| ![Python](https://img.shields.io/badge/-Python-3776AB?style=flat-square&logo=python&logoColor=white) | **Scripting** | Data transformation, database operations |
| ![Apache Spark](https://img.shields.io/badge/-Apache_Spark-E25A1C?style=flat-square&logo=apache-spark&logoColor=white) | **Distributed Processing** | PySpark for big data transformations |
| ![Power BI](https://img.shields.io/badge/-Power_BI-F2C811?style=flat-square&logo=power-bi&logoColor=black) | **Visualization** | Business intelligence dashboards |

---

## âœ¨ Features

### Data Pipeline Capabilities
- âœ… **Multi-source ingestion**: HTTP (GitHub CSVs), SQL, and NoSQL databases
- âœ… **Medallion architecture**: Industry-standard Bronze/Silver/Gold pattern
- âœ… **Data enrichment**: MongoDB integration for category translation
- âœ… **Scalable processing**: Apache Spark via Databricks for distributed computing
- âœ… **Analytics-ready**: Synapse Analytics for BI and reporting
- âœ… **Automated orchestration**: Azure Data Factory pipelines

### Business Metrics Calculated
- ğŸ“¦ Order delivery performance and delays
- ğŸ’° Payment analysis by type and installments
- â­ Customer review sentiment analysis
- ğŸ“ Geographic distribution of orders
- ğŸª Seller and product performance metrics

### Data Quality & Governance
- ğŸ” Data validation and cleansing
- ğŸ—‚ï¸ Schema standardization
- ğŸ“Š Data lineage tracking
- ğŸ”’ Secure credential management

---

## ğŸ“Š Dataset

**Olist Brazilian E-Commerce Dataset**

The project uses real-world e-commerce data from Olist, containing:

| Dataset | Records | Description |
|---------|---------|-------------|
| `olist_orders_dataset.csv` | 99,441 | Order information and status |
| `olist_order_items_dataset.csv` | 112,650 | Products within orders |
| `olist_customers_dataset.csv` | 99,441 | Customer demographics |
| `olist_products_dataset.csv` | 32,951 | Product catalog |
| `olist_sellers_dataset.csv` | 3,095 | Seller information |
| `olist_order_payments_dataset.csv` | 103,886 | Payment details |
| `olist_order_reviews_dataset.csv` | 99,224 | Customer reviews |
| `olist_geolocation_dataset.csv` | 1,000,163 | Geographic coordinates |
| `product_category_name_translation.csv` | 71 | Category translations |

**Total Data Volume**: ~1.3M records

---

## ğŸš€ Getting Started

### Prerequisites

- **Azure Account** (Free tier available)
- **Python 3.8+**
- **Git**
- Basic knowledge of SQL and Python
- (Optional) Power BI Desktop for visualizations

### Azure Resources Required

1. **Resource Group**
2. **Azure Data Lake Storage Gen2**
3. **Azure Data Factory**
4. **Azure Databricks Workspace**
5. **Azure Synapse Analytics**
6. **MongoDB Atlas** (Free tier)
7. **MySQL Database** (Free tier options available)

### Installation & Setup

#### 1. Clone the Repository
```bash
git clone https://github.com/siddharths060/DataFlowX.git
cd DataFlowX
```

#### 2. Set Up Azure Resources
```bash
# Login to Azure
az login

# Create resource group
az group create --name DataFlowX-RG --location eastus

# Create storage account with Data Lake Gen2
az storage account create \
  --name dataflowxstorage \
  --resource-group DataFlowX-RG \
  --location eastus \
  --sku Standard_LRS \
  --kind StorageV2 \
  --hierarchical-namespace true
```

#### 3. Configure Database Connection
Update credentials in `Database/connect_to_database.py`:
```python
hostname = "your-mysql-host"
database = "your-database-name"
username = "your-username"
password = "your-password"
port = "your-port"
```

#### 4. Upload Data to Bronze Layer
- Upload CSV files from `Data/` folder to Azure Data Lake Bronze container
- Or configure Data Factory to ingest from GitHub

#### 5. Configure MongoDB
- Set up MongoDB Atlas cluster
- Import `product_category_name_translation.csv`
- Update connection strings in Databricks notebooks

---

## ğŸ“‚ Project Structure

```
DataFlowX/
â”‚
â”œâ”€â”€ Data/                                    # Raw datasets
â”‚   â”œâ”€â”€ olist_customers_dataset.csv
â”‚   â”œâ”€â”€ olist_orders_dataset.csv
â”‚   â”œâ”€â”€ olist_order_items_dataset.csv
â”‚   â”œâ”€â”€ olist_order_payments_dataset.csv
â”‚   â”œâ”€â”€ olist_order_reviews_dataset.csv
â”‚   â”œâ”€â”€ olist_products_dataset.csv
â”‚   â”œâ”€â”€ olist_sellers_dataset.csv
â”‚   â”œâ”€â”€ olist_geolocation_dataset.csv
â”‚   â””â”€â”€ product_category_name_translation.csv
â”‚
â”œâ”€â”€ Database/                                # Database scripts
â”‚   â”œâ”€â”€ connect_to_database.py              # MySQL connection test
â”‚   â””â”€â”€ add_table_and_values_to_database.py # Data upload script
â”‚
â”œâ”€â”€ LICENSE                                  # Project license
â””â”€â”€ README.md                                # This file
```

---

## ğŸ”„ Pipeline Workflow

### Step 1: Data Ingestion (Azure Data Factory)
```
Sources â†’ ADF Pipelines â†’ Bronze Layer (ADLS Gen2)
```
- HTTP connector for GitHub CSV files
- SQL connector for MySQL database
- MongoDB connector for enrichment data

### Step 2: Data Transformation (Databricks - Bronze â†’ Silver)
```python
# Sample PySpark transformation
from pyspark.sql import functions as F

# Read from Bronze
df_orders = spark.read.parquet("/mnt/bronze/orders")

# Clean and transform
df_cleaned = df_orders \
    .dropDuplicates() \
    .na.drop() \
    .withColumn("order_date", F.to_date("order_purchase_timestamp")) \
    .withColumn("delivery_delay", 
                F.datediff("order_delivered_customer_date", 
                          "order_estimated_delivery_date"))

# Write to Silver
df_cleaned.write.mode("overwrite").parquet("/mnt/silver/orders")
```

### Step 3: Data Enrichment (MongoDB Integration)
- Join product data with category translations
- Standardize category names from Portuguese to English

### Step 4: Analytics Aggregation (Silver â†’ Gold)
```python
# Calculate business metrics
order_metrics = df_orders \
    .groupBy("order_status", "customer_state") \
    .agg(
        F.count("order_id").alias("total_orders"),
        F.avg("delivery_delay").alias("avg_delay_days"),
        F.sum("payment_value").alias("total_revenue")
    )

# Write to Gold layer
order_metrics.write.mode("overwrite").parquet("/mnt/gold/order_metrics")
```

### Step 5: Serving Layer (Synapse Analytics)
```sql
-- Create external table in Synapse
CREATE EXTERNAL TABLE gold.order_metrics
WITH (
    LOCATION = '/gold/order_metrics/',
    DATA_SOURCE = AzureDataLake,
    FILE_FORMAT = ParquetFormat
);
```

---

## ğŸ“ˆ Business Insights Enabled

1. **Order Analytics**
   - Delivery performance tracking
   - Order status distribution
   - Peak ordering periods

2. **Customer Insights**
   - Geographic customer distribution
   - Customer satisfaction scores
   - Repeat customer analysis

3. **Financial Metrics**
   - Revenue by product category
   - Payment method preferences
   - Installment payment trends

4. **Operational Excellence**
   - Seller performance rankings
   - Delivery delay analysis
   - Inventory turnover rates

---

## ğŸ“ Learning Outcomes

This project demonstrates proficiency in:

- â˜‘ï¸ **Azure Cloud Services**: Data Factory, Databricks, Synapse, ADLS Gen2
- â˜‘ï¸ **Big Data Processing**: Apache Spark, PySpark, distributed computing
- â˜‘ï¸ **Data Architecture**: Medallion pattern, data lake design
- â˜‘ï¸ **ETL/ELT**: Pipeline orchestration, data transformation
- â˜‘ï¸ **Multi-source Integration**: HTTP, SQL, NoSQL data sources
- â˜‘ï¸ **Data Modeling**: Dimensional modeling, star schema
- â˜‘ï¸ **SQL & Python**: Advanced querying and scripting
- â˜‘ï¸ **DevOps**: CI/CD for data pipelines (optional extension)

---

## ğŸ”® Future Enhancements

- [ ] Implement CI/CD with Azure DevOps
- [ ] Add real-time streaming with Event Hubs
- [ ] Machine learning integration (Azure ML)
- [ ] Advanced monitoring with Azure Monitor
- [ ] Data quality framework (Great Expectations)
- [ ] Cost optimization strategies
- [ ] Delta Lake integration for ACID transactions
- [ ] Automated testing for data pipelines

---

## ğŸ“š Resources & References

### Tutorials
- [End-to-End Big Data Engineering Project - Part 1](https://www.youtube.com/watch?v=K0KPFoWwvwg)
- [End-to-End Big Data Engineering Project - Part 2](https://www.youtube.com/watch?v=zxYyJkNB3Q0)

### Documentation
- [Azure Data Factory Documentation](https://docs.microsoft.com/azure/data-factory/)
- [Azure Databricks Documentation](https://docs.microsoft.com/azure/databricks/)
- [Azure Synapse Analytics Documentation](https://docs.microsoft.com/azure/synapse-analytics/)
- [Medallion Architecture Pattern](https://docs.databricks.com/lakehouse/medallion.html)

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the terms specified in the [LICENSE](LICENSE) file.

---

## ğŸ‘¤ Author

**Siddharth**

- GitHub: [@siddharths060](https://github.com/siddharths060)
- Project: [DataFlowX](https://github.com/siddharths060/DataFlowX)

---

## â­ Show Your Support

If this project helped you learn Azure data engineering, please give it a â­ï¸!

---

<div align="center">

**Built with â¤ï¸ using Azure Cloud Services**

![Microsoft Azure](https://img.shields.io/badge/Built_on-Microsoft_Azure-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white)

</div>
